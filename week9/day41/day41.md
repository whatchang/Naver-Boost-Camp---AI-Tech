<!--
구조
*
    *
        * <br>
            &nbsp; - &nbsp; <br>
                &nbsp;&nbsp;&nbsp;&nbsp; ‣ &nbsp; <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * &nbsp; <br>
-->

# Day 

## 목차 

1. [공부한 내용 정리](#1-공부한-내용-정리)

2. [과제 정리](#2-과제-정리)

3. [피어세션 정리](#3-피어세션-정리)

4. [학습 회고](#4-학습-회고)

## 1. 공부한 내용 정리

* 강의 7강 듣기 <- 주말에 정리할 예정

* KLUE 대회 관련
    * 어제 시도한 방식이 잘 안되었는데 validation으로 평가해봤을때 깊게 학습이 안 되는것 같다(epoch 3이후부터는 micro_f1이 비슷함).
    * 위를 해결하기 위해서 gradient clipping을 사용해보면 어떨까 싶다.
    * pre-train MLM에 사용되는 데이터가 original보다 undersampling(about class 0)가 더 좋은 성능을 보여주었다. -> micro_f1 : 69.654, auprc : 73.242

<br>

## 2. 과제 정리

없음

<br>

## 3. 피어세션 정리

* 참석자
    * 김신곤, 김재영, 박세진, 손희락, 심우창, 이상준, 전상민

* 다음 주 모더레이터
    * 김재영, 이상준

* 오전 게더 미팅
    * 누구 한명이 안오는게 문제가 아니라 한사람씩 안오기 때문에 문제인 것 같다.
        * 재영님 : 자기 할일을 말하는 것이기 때문에 굳이 안하는 것도 괜찮은 것 같다.
        * 우창님 : 하고 싶은 사람끼리 하면 좋을 것 같다.
        * 신곤님 : 할거면 하고 안할거면 안하는 것이 좋은 것 같다.
        * 상준님 : 더 열심히 하거나 그런 것 같지는 않다. 오히려 쓰는 것이 크게 좋은지 모르겠다.
        * 상민님 : 우창님 의견이랑 동일하다.
    * 선택적으로 참여하고 싶은 인원만 참여하도록
        * 세진, 희락, 우창, 상민 매일 아침 오전 10시 10분에 게더에서 만나요!
        * 혹시 오시고 싶으신 분들 또 계시면 언제든지 방문하셔도 좋습니당 :)
        * 10시 10분에서 더 늦게 오면 얄짤 없이 그냥 빼고 진행
* 강의 리뷰
    * 6장, 7장

* 9주차 팀 회고록 작성
    * 팀 회고록

* 대회 관련
    * 어제의 office hour를 참고해서 AEDA 등을 시도해보기
    * 특정 label을 바꿔가면서 시도해보는 것도 좋을 것 같음
    * early stopping은 희락님께서 이미 코드에 적용하셨음
    * BERT base 3epoch으로도 괜찮은 성능이 나온 것을 보니, model을 굳이 현재 시도하고 있는 KLUE-RoBERTa만으로 fix해야할 필요는 없을 것 같음
* 다음 주 월요일 줌은 몇 시?
    * 5시에 줌에서 만나용
* 각자 해야할 일
    * issue에 각자 질문 update하기!

<br>

## 4. 학습 회고

* 오늘은 어제와 다르게 좀 더 집중해서 들어보려고 했는데 쉽지 않았다. 아무래도 그냥 듣기만 해서 그런것 같다. 주말에 정리하면서 듣다보면 좀 더 강의에 집중하게 될 것 같다.
* 오전에 MLM을 따로 pre-train 시켜서 fine-tuning한 후 제출을 했는데 생각했던 것보다 성능이 별로 안 좋았다. 나중에 확인해보니 MLM을 적용시킨 pre-train모델을 가지고 fine-tuning을 한게 아니였다;;; 그래서 적용한 모델을 가지고 fine-tuning을 해보니!!! 두둥!!! 성능이 더 떨어졌다 ㅋㅋㅋㅋㅋ. train dataset을 가지고 MLM을 적용시키지 않은 모델이 적용시킨 모델보다 약 0.1% 더 높았다 ㅎㅎㅎㅎ
* 그 후에 original dataset으로 MLM을 시키지 않고 undersampling(about class 0)한 dataset을 가지고 MLM을 한 후에 fine-tuning을 해보니 성능이 2% 올랐다~~~!!!! -> 이것을 통해서 추측해본 점은 pre-train시 dataset의 분포가 비슷한게 더 좋은 것 같다. -> 의문점 : 그런데 음.... pre-train MLM일때는 label을 통해서하는 작업이 없으니까 상관없지 않나? 어짜피 MLM자체가 unlabeled data를 사용하는 건데...? 뭐징?
<br>