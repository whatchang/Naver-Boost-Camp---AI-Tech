<!--
êµ¬ì¡°
*
    *
        * <br>
            &nbsp; - &nbsp; <br>
                &nbsp;&nbsp;&nbsp;&nbsp; â€£ &nbsp; <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * &nbsp; <br>
-->

# week8-5 

## ëª©ì°¨ 

1. [ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬](#1-ê³µë¶€í•œ-ë‚´ìš©-ì •ë¦¬)

2. [í•™ìŠµ íšŒê³ ](#2-í•™ìŠµ-íšŒê³ )

## 1. ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬

BERT ë…¼ë¬¸ ì •ë¦¬

Abstract
- BERTëŠ” unlabeled textì— ëŒ€í•´ì„œ ì–‘ë°©í–¥ pre-trainì„ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì–´ì¡Œë‹¤.
- BERTëŠ” í•œ ê°œì˜ output layerë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì—¬ëŸ¬ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.

Introduction
- ì´ì „ê¹Œì§€ì˜ LMì€ feature-basedì™€ fine-tuningì˜ ì „ëµì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ë“¤ì´ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ í•´ë‹¹ ëª¨ë¸ë“¤ì€ unidirectionalì´ê±°ë‚˜ ì™„ë²½í•œ bidirectional ëª¨ë¸ì´ ì•„ë‹ˆì˜€ë‹¤. -> feature-based(Elmo), fine-tuning(GPT-1)
- ìœ„ì™€ ê°™ì€ ëª¨ë¸ì˜ í•œê³„ëŠ” ë‹¨ì–´ì˜ ê´€ê³„ì— ëŒ€í•´ì„œ ë¬¸ë§¥ ì •ë³´ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œë‹¤.
- BERTëŠ” ìœ„ì™€ ê°™ì€ í•œê³„ë¥¼ ê·¹ë³µí•œ ëª¨ë¸ì´ë‹¤.
- BERTëŠ” MLM(Masked Language Model)ë°©ì‹ì„ pre-trainingì— ì‚¬ìš©í•˜ë¯€ë¡œì¨ ì´ì „ì˜ unidirectionalì˜ í•œê³„ì ì„ ì™„í™”ì‹œì¼°ë‹¤. * MLMì€ ì¢Œìš° ë¬¸ë§¥(ë‹¨ì–´)ë¥¼ ì˜ ë…¹ì—¬ì„œ í•™ìŠµí•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤.
- BERTëŠ” NSP(Next Sentence Prediction)ì´ë¼ëŠ” ë°©ì‹ë„ pre-trainì‹œ ì‚¬ìš©ì„ í•œë‹¤.

Conclusion
- ì €ìì˜ ê¸°ì—¬í•œ ì ì€ deep bidirectional architecturesë°©ì‹ì„ ì¼ë°˜í™”ì‹œí‚¨ ê²ƒì´ë‹¤.

ë‚˜ì˜ ìƒê°(Abstractì™€ Introduction, Conclusionì„ ì½ê³  ë‚œ í›„)
- ë¬´ì—‡ì„ í•´ê²°í•˜ê³  ì‹¶ì€ ê²ƒì¸ê°€?
    - ì´ì „ê¹Œì§€ì˜ í•œê³„(unidirectional)ë¥¼ ê·¹ë³µí•˜ê³ ì í–ˆë‹¤.
- ì´ì „ ëª¨ë¸ê³¼ ë‹¬ë¼ì§„ ì ì€?
    - MLMê³¼ NSP ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ pre-trainingì—ì„œ bidirectionalí•˜ê²Œ í•™ìŠµí•˜ì—¬ ì¢Œìš° ë¬¸ë§¥(ë‹¨ì–´)ì˜ ê´€ê³„ë¥¼ ë°˜ì˜ì‹œì¼°ë‹¤.
- í•µì‹¬ í‚¤ì›Œë“œ
    - Bidirection
    - MLM
    - NSP
    - Encoder
- ë…¼ë¬¸ ì§„í–‰ ë°©í–¥(íë¦„) ì˜ˆìƒ
    - MLMê³¼ NSPì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ì„¤ëª…ì„ í•˜ê³  ì´ê²ƒì´ ì¢Œìš° ë¬¸ë§¥ì„ ì´í•´í•˜ëŠ”ë° ì–´ë–¤ ë„ì›€ì„ ì£¼ì—ˆëŠ”ì§€ ì„¤ëª…í•  ê²ƒ ê°™ë‹¤.
    - ì´ì „ ëª¨ë¸ì— ë¹„í•´ì„œ BERTì˜ ì„±ëŠ¥ì´ ì–´ëŠì •ë„ ì¢‹ì•„ì¡ŒëŠ”ì§€ ë¹„êµí•˜ê³  bidirectional language modelì˜ ê°•ì ì„ ì„¤ëª…í•  ê²ƒ ê°™ë‹¤.
	
Related Work
- Unsupervised Feature-based Approaches

- Unsupervised Fine-tuning Approaches

- Transfer Learning from Supervised Data

BERT ë™ì‘ ì›ë¦¬
- BERTëŠ” ë¨¼ì € pre-trainìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ initializationì„ í•˜ê³  fine-tuningì„ í†µí•´ì„œ í•´ë‹¹ taskì— ë§ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµì‹œì¼œì¤€ë‹¤.
- Architecture
    - Base : num of layer 12, hidden size 768, number of self-attention head 12 => total parameter 110M
    - Large : num of layer 24, hidden size 1024, number of self-attention heads 16 => total parameter 340M
        * ìœ„ì˜ ê³„ì‚°ì— ëŒ€í•œ ìì„¸í•œ ê²ƒì€ https://github.com/google-research/bert/issues/656 ì°¸ê³ 
- Input/Output Representations
    - WordPiece embeddingsì„ ì‚¬ìš©
    - Input sequenceì˜ ë§¨ ì²˜ìŒì€ CLSë¼ëŠ” special tokenì„ ì‚¬ìš©í•œë‹¤.
    - 2ê°œì˜ sentenceë¥¼ í•˜ë‚˜ì˜ inputìœ¼ë¡œ ë°›ì„ë•Œ ì´ê²ƒì„ êµ¬ë³„í•˜ê¸° ìœ„í•´ì„œ SEPë¼ëŠ” special tokenì„ ì‚¬ìš©í•œë‹¤. ì´ê²ƒì— ë”°ë¼ segment embeddingì„ í•´ì¤€ë‹¤.
    - Position embeddingë„ ì‚¬ìš©í•œë‹¤
    - ì´ 3ê°€ì§€ì˜ embedding ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.
- Pre-training BERT 
    - MLM(Masked Language Model) 
        * deep bidirectional representationì„ ìœ„í•´ì„œ inputì˜ ë¬´ì‘ìœ„ë¡œ maskingì„ í•œë‹¤. ì´ë•Œ inputì— ëŒ€í•œ masking ë¹„ìœ¨ì€ 15%ì´ë‹¤.
        * maskingëœ ë¶€ë¶„ì€ 80%ê°€ mask tokenìœ¼ë¡œ ë°”ë€Œê³  10%ëŠ” ë¬´ì‘ìœ„ë¡œ ë°”ë€Œë©° ë‚¨ì€ 10%ëŠ” ë³€í™”ë˜ì§€ ì•ŠëŠ”ë‹¤.
    - NSP(Next Sentence Prediction)
        * 50%ì •ë„ëŠ” ì•ë’¤ ë¬¸ì¥ì´ ì„ íƒë˜ê³  50%ì •ë„ëŠ” ë¬´ì‘ìœ„ë¡œ 2ê°œì˜ ë¬¸ì¥ì´ ì¶”ì¶œëœë‹¤.
        * ì¶”ì¶œëœ ë¬¸ì¥ì´ ì—°ì†ì ì¸ì§€ì— ëŒ€í•´ì„œ êµ¬ë³„í•˜ëŠ” taskë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ í•™ìŠµì„ í•œë‹¤.
- Fine-tuning BERT
    - CLSëŠ” classfication(entailment or sentiment analysis)ì— ëŒ€í•œ taskë¥¼ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í•˜ë©° sequence tagging or question answeringê³¼ ê°™ì€ token level taskë“¤ì€ token representationì„ í†µí•´ì„œ taskë¥¼ ìˆ˜í–‰í•œë‹¤.
    - 
Experiments

Ablation Studies

ë‚´ ìƒê°(1íšŒë… í›„ BERTì— ëŒ€í•œ ìƒê°)
- í•µì‹¬ í‚¤ì›Œë“œ
- ì €ìì˜ ë§í•˜ê³ ì í•œ ê²ƒ
- ì¶”ê°€ë¡œ ë³´ê³  ì‹¶ì€ reference
- í™œìš© ë°©ì•ˆ ë° ë°œì „ ë°©í–¥ ìƒê°í•´ ë³´ê¸°


<br>

## 2. í•™ìŠµ íšŒê³ 

* ì˜¤ëŠ˜ BERT ë…¼ë¬¸ì„ ë‹¤ ì½ì„ ìƒê°ì´ì—ˆì§€ë§Œ ë‹¤ ì½ì§€ ëª»í•˜ì˜€ë‹¤ ã… ã… 
* ê·¸ë˜ë„ ì •ë¦¬í•˜ë©´ì„œ ì½ìœ¼ë‹ˆê¹Œ ë­”ê°€ ë” ì˜ ì½íˆëŠ” ëŠë‚Œì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì˜ì–´ëŠ”... ì–´ë ¤...ë‹¤... ã… ã… 
* ì´ë²ˆ ì£¼ë„ ìˆ˜ê³  ë§ì•˜ê³  ë‹¤ìŒ ëŒ€íšŒ í™”ì´íŒ…~~~!ğŸ‘

<br>