<!--
êµ¬ì¡°
*
    *
        * <br>
            &nbsp; - &nbsp; <br>
                &nbsp;&nbsp;&nbsp;&nbsp; â€£ &nbsp; <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * &nbsp; <br>
-->

# Day 

## ëª©ì°¨ 

1. [ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬](#1-ê³µë¶€í•œ-ë‚´ìš©-ì •ë¦¬)

2. [í•™ìŠµ íšŒê³ ](#2-í•™ìŠµ-íšŒê³ )

## 1. ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬

[ë…¼ë¬¸ì½ê¸°] - Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. "On the difficulty of training recurrent neural networks." ICML (3) 28 (2013): 1310-1318.

* Abstract
    * RNNê³„ì—´ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” gradient exploding/vanishingì— ëŒ€í•´ì„œ geometric & dynamical systems perspectiveë¥¼ í†µí•´ì„œ ì¢€ ë” ì´í•´í•˜ëŠ” ê²ƒ.
    * gradient exploding/vanishingì„ gradient norm clipping strategyë¥¼ ì´ìš©í•´ì„œ ë‹¤ë£¨ëŠ” ë²• ì œì‹œ
<br>

* Introduction
    * vanilla RNN êµ¬ì¡° ì„¤ëª… ë° BPTT 

<br>

* ë‚´ ìƒê°(Abstractì™€ Introductionë§Œ ì½ê³  ìƒê°í•´ë³´ê¸°)
    * ë¬´ì—‡ì„ í•´ê²°í•˜ê³  ì‹¶ì€ ê²ƒì¸ê°€? RNN êµ¬ì¡°ì—ì„œ ë°œìƒí•˜ëŠ” gradient exploding/vanishingë¬¸ì œë¥¼ ì´í•´í•˜ê³  í•´ê²°í•˜ê³  ì‹¶ì–´í•˜ëŠ” ê²ƒ ê°™ë‹¤.
    * í•µì‹¬ í‚¤ì›Œë“œ - gradient exploding/vanishing, norm clipping
    * ë…¼ë¬¸ ì§„í–‰ ë°©í–¥(íë¦„) - ì—¬ëŸ¬ ê´€ì ì—ì„œ gradient exploding/vanishingì„ ë¶„ì„í•˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ì†Œê°œí•˜ëŠ” ë°©ì‹ì¼ ê²ƒ ê°™ë‹¤.
    * ë‚´ê°€ ì´ ë…¼ë¬¸ì„ ì½ê³  ì–»ì–´ê°€ë ¤ëŠ” ê²ƒ - ì—¬ê¸°ì„œ ì†Œê°œí•œ ë°©ì‹ì„ í† ëŒ€ë¡œ explodingê³¼ vanishingì„ ì´í•´í•´ë³´ê³  LSTMì´ gradient exploding/vanishingì„ ì–´ë–»ê²Œ í•´ê²°í•˜ë ¤ê³  í–ˆëŠ”ì§€ë¥¼ ìƒê°í•´ë³´ë©´ì„œ LSTMì˜ í•œê³„ë¥¼ íŒŒì•…í•´ ë³¼ ìƒê°ì´ë‹¤.

<br>

* Exploding and vanishing gradients
    * Bengio et al. (1994)ì—ì„œ ì„¤ëª…ëœ gradient explodingê³¼ vanishingì— ëŒ€í•´ì„œ ì´ì•¼ê¸° í•´ì¤Œ
    * RNNì—ì„œ hidden stateì— ëŒ€í•œ ê°€ì¤‘ì¹˜ê°€ 1ë³´ë‹¤ í¬ë©´ exploding ì‘ìœ¼ë©´ vanishingì´ í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤.
    <img src='./img/vanishing.png'>
    * ìœ„ì˜ ì‹ì—ì„œ ì•„ë˜ë¥¼ ë§Œì¡±í• ë•Œ ğœ†1ì˜ ê°’ì´ ì»¤ì§ˆìˆ˜ë¡(ìµœëŒ“ê°’ì´ ë  ë•Œ) gradient vanishingì´ ë°œìƒí•œë‹¤.
    <img src='./img/vanishing1.png'>
    * ìœ„ì˜ ì¦ëª…ì— ëŒ€í•œ ìˆ˜ì‹ì€ ë…¼ë¬¸ 6,7ë²ˆì„ ë³´ë©´ ëœë‹¤.
    * gradient vanishingì€ ë°˜ëŒ€ë¡œ ğœ†1ì´ 1/ğ›¾ë³´ë‹¤ í¬ë‹¤ë©´ explodingëœë‹¤.
    *
<br>

* Dealing with the exploding and vanishing gradient 
    * dynamical systems ì´ë¡ ì— ëŒ€í•´ì„œ ìì„¸í•œ ë‚´ìš©ê³¼ ê³µì‹ì„ ì•Œê¸° ìœ„í•´ì„œ 'Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry, And Engineering (Studies in Nonlinearity). Studies in nonlinearity. Perseus Books Group, 1 edition.'ë¥¼ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•¨
    * teacher forcingì´ gradient explodingì„ ì¤„ì´ëŠ” ê¸°íšŒë¥¼ ì œê³µí•˜ì˜€ë‹¤. -> teacher forcingì´ ì–´ë–»ê²Œ gradient explodingì„ ì¤„ì˜€ëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ì€ 'Doya, K. (1993). Bifurcations of recurrent neural net- works in gradient descent learning. IEEE Transac- tions on Neural Networks, 1, 75â€“80.ê³¼ 'Doya, K. and Yoshizawa, S. (1991). Adaptive synchro- nization of neural and physical oscillators. In J. E. Moody, S. J. Hanson, and R. Lippmann, editors, NIPS, pages 109â€“116. Morgan Kaufmann' ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.  
    * LSTMì—ì„œëŠ” íŠ¹ë³„í•œ êµ¬ì¡°(cell state unit)ë¥¼ í†µí•´ì„œ gradient vanishingì— ëŒ€í•´ì„œ ë‹¤ë£¨ì—ˆë‹¤ê³  í•œë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” gradient explodingì˜ í•´ê²°ë²•ì— ëŒ€í•´ì„œëŠ” ì„¤ëª…ì´ ì—†ë‹¤ê³  í•œë‹¤.
    * structural dampingì´ explodingë¬¸ì œì— ë„ì›€ì„ ì¤€ë‹¤ê³  í•œë‹¤. structural dampingì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ 'Martens, J. and Sutskever, I. (2011). Learning recur- rent neural networks with Hessian-free optimization. In Proc. ICMLâ€™2011. ACM'ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.
    * Echo state networkëŠ” hiddent stateì™€ í•´ë‹¹ time stempì˜ inputì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œì¨ gradient exploding/vanishing ë¬¸ì œë¥¼ íšŒí”¼í•˜ì˜€ë‹¤ê³  í•œë‹¤. í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” 'aeger, H. and Haas, H. (2004). Harnessing nonlinear- ity: predicting chaotic systems and saving energy in wireless telecommunication. Science, 304(5667), 78â€“80' ì‚´í´ë³´ë©´ ëœë‹¤.
    * ìœ„ì˜ ëª¨ë¸ì˜ í™•ì¥ëœ ë²„ì „ì´ leaky integration unitì´ë‹¤. ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ 'Jaeger, H., Lukosevicius, M., Popovici, D., and Siew- ert, U. (2007). Optimization and applications of echo state networks with leaky- integrator neurons. Neural Networks, 20(3), 335â€“352.'ì„ ì½ì–´ë³´ì.
    * clipping the gradient's temporal componentsë¥¼ ì´ìš©í•˜ì—¬ gradient exploding/vanishingì„ í•´ê²°í•˜ë ¤ëŠ” ì ‘ê·¼ ë°©ì‹ë„ ìˆë‹¤. 'Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011). Empirical evaluation and combination of advanced language modeling tech- niques. In Proc. 12th annual conference of the in- ternational speech communication association (IN- TERSPEECH 2011)'ê³¼ 'Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology'ì„ ì°¸ê³ í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.
    * Scaling down the gradients -> gradient explodingì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€
    <img src=./img/scailing_down.png>
    * Regularization - Backpropagationë•Œ ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì— ëŒ€í•œ ì •ê·œí™” -> gradient vanishingì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤. => í•´ê²°í•  ìˆ˜ ìˆëŠ” ìš”ì¸ì— ëŒ€í•œ ì„¤ëª…ì€ ìˆ˜ì‹ê³¼ ì˜ì–´ë¥¼ ì˜ ì´í•´í•˜ì§€ ëª»í•´ì„œ ì„¤ëª… ë¶ˆê°€ëŠ¥ ã… ã… (í•´ë‹¹ ë…¼ë¬¸ 7page 3.3 vanishing gradient regularizationì„ ì½ì–´ë³´ë©´ ëœë‹¤.)

<br>

* Experiments and results

<br>

* Summary and conclusions

<br>

* ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œ ëª¨ë¥´ëŠ” í˜¹ì€ ì¢€ ë” ì•Œì•„ì•¼ í•  ê²ƒ ê°™ì€ ê°œë…
    * [ëª¨ë¥´ëŠ”] jacobian matrices
    * [ëª¨ë¥´ëŠ”] basin of attraction


<br>

## 2. í•™ìŠµ íšŒê³ 

<br>