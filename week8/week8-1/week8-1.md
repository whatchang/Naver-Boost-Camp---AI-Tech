<!--
êµ¬ì¡°
*
    *
        * <br>
            &nbsp; - &nbsp; <br>
                &nbsp;&nbsp;&nbsp;&nbsp; â€£ &nbsp; <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * &nbsp; <br>
-->

# Day 

## ëª©ì°¨ 

1. [ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬](#1-ê³µë¶€í•œ-ë‚´ìš©-ì •ë¦¬)

2. [í•™ìŠµ íšŒê³ ](#2-í•™ìŠµ-íšŒê³ )

## 1. ê³µë¶€í•œ ë‚´ìš© ì •ë¦¬

[ë…¼ë¬¸ì½ê¸°] - Pascanu, Razvan, Tomas Mikolov, and Yoshua Bengio. "On the difficulty of training recurrent neural networks." ICML (3) 28 (2013): 1310-1318.

* Abstract
    * RNNê³„ì—´ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” gradient exploding/vanishingì— ëŒ€í•´ì„œ geometric & dynamical systems perspectiveë¥¼ í†µí•´ì„œ ì¢€ ë” ì´í•´í•˜ëŠ” ê²ƒ.
    * gradient exploding/vanishingì„ gradient norm clipping strategyë¥¼ ì´ìš©í•´ì„œ ë‹¤ë£¨ëŠ” ë²• ì œì‹œ
<br>

* Introduction
    * vanilla RNN êµ¬ì¡° ì„¤ëª… ë° BPTT 

<br>

* ë‚´ ìƒê°(Abstractì™€ Introductionë§Œ ì½ê³  ìƒê°í•´ë³´ê¸°)
    * ë¬´ì—‡ì„ í•´ê²°í•˜ê³  ì‹¶ì€ ê²ƒì¸ê°€? RNN êµ¬ì¡°ì—ì„œ ë°œìƒí•˜ëŠ” gradient exploding/vanishingë¬¸ì œë¥¼ ì´í•´í•˜ê³  í•´ê²°í•˜ê³  ì‹¶ì–´í•˜ëŠ” ê²ƒ ê°™ë‹¤.
    * í•µì‹¬ í‚¤ì›Œë“œ - gradient exploding/vanishing, norm clipping
    * ë…¼ë¬¸ ì§„í–‰ ë°©í–¥(íë¦„) - ì—¬ëŸ¬ ê´€ì ì—ì„œ gradient exploding/vanishingì„ ë¶„ì„í•˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì „ëµì„ ì†Œê°œí•˜ëŠ” ë°©ì‹ì¼ ê²ƒ ê°™ë‹¤.
    * ë‚´ê°€ ì´ ë…¼ë¬¸ì„ ì½ê³  ì–»ì–´ê°€ë ¤ëŠ” ê²ƒ - ì—¬ê¸°ì„œ ì†Œê°œí•œ ë°©ì‹ì„ í† ëŒ€ë¡œ explodingê³¼ vanishingì„ ì´í•´í•´ë³´ê³  LSTMì´ gradient exploding/vanishingì„ ì–´ë–»ê²Œ í•´ê²°í•˜ë ¤ê³  í–ˆëŠ”ì§€ë¥¼ ìƒê°í•´ë³´ë©´ì„œ LSTMì˜ í•œê³„ë¥¼ íŒŒì•…í•´ ë³¼ ìƒê°ì´ë‹¤.

<br>

* Exploding and vanishing gradients
    * Bengio et al. (1994)ì—ì„œ ì„¤ëª…ëœ gradient explodingê³¼ vanishingì— ëŒ€í•´ì„œ ì´ì•¼ê¸° í•´ì¤Œ
    * RNNì—ì„œ hidden stateì— ëŒ€í•œ ê°€ì¤‘ì¹˜ê°€ 1ë³´ë‹¤ í¬ë©´ exploding ì‘ìœ¼ë©´ vanishingì´ í˜„ìƒì´ ë‚˜íƒ€ë‚œë‹¤.
    <img src='./img/vanishing.png'><br>
    * ìœ„ì˜ ì‹ì—ì„œ ì•„ë˜ë¥¼ ë§Œì¡±í• ë•Œ ğœ†1ì˜ ê°’ì´ ì»¤ì§ˆìˆ˜ë¡(ìµœëŒ“ê°’ì´ ë  ë•Œ) gradient vanishingì´ ë°œìƒí•œë‹¤.<br>
    <img src='./img/vanishing1.png'><br>
    * ìœ„ì˜ ì¦ëª…ì— ëŒ€í•œ ìˆ˜ì‹ì€ ë…¼ë¬¸ 6,7ë²ˆì„ ë³´ë©´ ëœë‹¤.
    * gradient vanishingì€ ë°˜ëŒ€ë¡œ ğœ†1ì´ 1/ğ›¾ë³´ë‹¤ í¬ë‹¤ë©´ explodingëœë‹¤.
<br>

* Dealing with the exploding and vanishing gradient 
    * dynamical systems ì´ë¡ ì— ëŒ€í•´ì„œ ìì„¸í•œ ë‚´ìš©ê³¼ ê³µì‹ì„ ì•Œê¸° ìœ„í•´ì„œ <br>
    <br>'Nonlinear Dynamics And Chaos: With Applications To Physics, Biology, Chemistry, And Engineering (Studies in Nonlinearity). Studies in nonlinearity. Perseus Books Group, 1 edition.'ë¥¼ ì½ì–´ë³´ëŠ” ê²ƒì„ ì¶”ì²œí•¨<br><br>
    * teacher forcingì´ gradient explodingì„ ì¤„ì´ëŠ” ê¸°íšŒë¥¼ ì œê³µí•˜ì˜€ë‹¤. <br><br>
    -> teacher forcingì´ ì–´ë–»ê²Œ gradient explodingì„ ì¤„ì˜€ëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ì€ 'Doya, K. (1993). Bifurcations of recurrent neural net- works in gradient descent learning. IEEE Transac- tions on Neural Networks, 1, 75â€“80.ê³¼ 'Doya, K. and Yoshizawa, S. (1991). Adaptive synchro- nization of neural and physical oscillators. In J. E. Moody, S. J. Hanson, and R. Lippmann, editors, NIPS, pages 109â€“116. Morgan Kaufmann' ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.  <br><br>
    * LSTMì—ì„œëŠ” íŠ¹ë³„í•œ êµ¬ì¡°(cell state unit)ë¥¼ í†µí•´ì„œ gradient vanishingì— ëŒ€í•´ì„œ ë‹¤ë£¨ì—ˆë‹¤ê³  í•œë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” gradient explodingì˜ í•´ê²°ë²•ì— ëŒ€í•´ì„œëŠ” ì„¤ëª…ì´ ì—†ë‹¤ê³  í•œë‹¤.<br><br>
    * structural dampingì´ explodingë¬¸ì œì— ë„ì›€ì„ ì¤€ë‹¤ê³  í•œë‹¤. structural dampingì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€<br><br>
    'Martens, J. and Sutskever, I. (2011). Learning recur- rent neural networks with Hessian-free optimization. In Proc. ICMLâ€™2011. ACM'ì„ ì°¸ê³ í•˜ë©´ ë  ê²ƒ ê°™ë‹¤.<br><br>
    * Echo state networkëŠ” hiddent stateì™€ í•´ë‹¹ time stempì˜ inputì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œì¨ gradient exploding/vanishing ë¬¸ì œë¥¼ íšŒí”¼í•˜ì˜€ë‹¤ê³  í•œë‹¤. <br><br>í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” 'aeger, H. and Haas, H. (2004). Harnessing nonlinear- ity: predicting chaotic systems and saving energy in wireless telecommunication. Science, 304(5667), 78â€“80' ì‚´í´ë³´ë©´ ëœë‹¤.<br><br>
    * ìœ„ì˜ ëª¨ë¸ì˜ í™•ì¥ëœ ë²„ì „ì´ leaky integration unitì´ë‹¤. <br><br>ì¢€ ë” ìì„¸í•œ ë‚´ìš©ì€ 'Jaeger, H., Lukosevicius, M., Popovici, D., and Siew- ert, U. (2007). Optimization and applications of echo state networks with leaky- integrator neurons. Neural Networks, 20(3), 335â€“352.'ì„ ì½ì–´ë³´ì.<br><br>
    * clipping the gradient's temporal componentsë¥¼ ì´ìš©í•˜ì—¬ gradient exploding/vanishingì„ í•´ê²°í•˜ë ¤ëŠ” ì ‘ê·¼ ë°©ì‹ë„ ìˆë‹¤. <br><br>'Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011). Empirical evaluation and combination of advanced language modeling tech- niques. In Proc. 12th annual conference of the in- ternational speech communication association (IN- TERSPEECH 2011)'ê³¼ 'Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology'ì„ ì°¸ê³ í•´ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤.<br><br>
    * Scaling down the gradients -> gradient explodingì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€
    <img src=./img/scailing_down.png><br>
    * Regularization - Backpropagationë•Œ ì‚¬ìš©ë˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì— ëŒ€í•œ ì •ê·œí™” -> gradient vanishingì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤. <br><br> => í•´ê²°í•  ìˆ˜ ìˆëŠ” ìš”ì¸ì— ëŒ€í•œ ì„¤ëª…ì€ ìˆ˜ì‹ê³¼ ì˜ì–´ë¥¼ ì˜ ì´í•´í•˜ì§€ ëª»í•´ì„œ ì„¤ëª… ë¶ˆê°€ëŠ¥ ã… ã… (í•´ë‹¹ ë…¼ë¬¸ 7page 3.3 vanishing gradient regularizationì„ ì½ì–´ë³´ë©´ ëœë‹¤.)

<br>

* Experiments and results
    * Temporal order problem 
        * ì•„ë˜ëŠ” MSGD, MSGD-C, MSGD-CRì— ëŒ€í•œ sequenceê¸¸ì´ì— ë”°ë¥¸ temporal order problemì„ í•´ê²° ë¹„ìœ¨ì´ë‹¤.
        <img src=./img/graph1.png>
        * ìœ„ì˜ ê·¸ë¦¼ì—ì„œ MSGD-CëŠ” batch stochastic gradient descentì—ì„œ clipping strategyë¥¼ ì‚¬ìš©í•œ ê²ƒì´ë©° ì—¬ê¸°ì— Rì´ ì¶”ê°€ëœ MSGD-CRì€ MSGD-Cì—ì„œ regularization termì´ ì¶”ê°€ëœ ê²ƒì´ë‹¤.
        * CRë°©ì‹ì„ ì‚¬ìš©í•œë‹¤ë©´ sigmoid, tanh, smart-tanhë“±ì˜ í™œì„±í™” í•¨ìˆ˜ì—ì„œ rate of successê°€ ì¢‹ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 
        * ! __ì—¬ê¸°ì„œ í° ë¬¸ì œ__ rate of successê°€ ë¬´ì—‡ì¸ì§€ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•¨ ã… ã… 
    <br>

    * Other pathological tasks
        * SGD-CRì€ addition problem, multiplication problem, 3-bit temporal order problem, random permutation problem ë“±ì„ í•´ê²°í•˜ì˜€ë‹¤.
<br>

* Summary and conclusions
    * gradient exploding - clipping
    * gradient vanishing - regularization <- backpropagationë•Œ gradientê°€ ì‚¬ë¼ì§€ì§€ ì•Šë„ë¡ ì •ê·œí™”ë¥¼ í•´ì¤€ë‹¤. jacobian matricesì´ normì„ ê´€ë ¨ëœ ë°©í–¥ìœ¼ë¡œ ë³´ì¡´í•˜ë„ë¡ í•´ì¤€ë‹¤. <- ì´ ë¶€ë¶„ì— ëŒ€í•´ì„œ í•´ì„ì„ ì˜ ëª»í•˜ê² ë‹¤ ã… ã… , ì›ë³¸ì€ ì•„ë˜ ë¬¸ì¥ì´ë‹¤.
        * <img src=./img/ex1.png>

<br>

* ë…¼ë¬¸ì„ ì½ìœ¼ë©´ì„œ ëª¨ë¥´ëŠ” í˜¹ì€ ì¢€ ë” ì•Œì•„ì•¼ í•  ê²ƒ ê°™ì€ ê°œë…
    * [ëª¨ë¥´ëŠ”] jacobian matrices
    * [ëª¨ë¥´ëŠ”] basin of attraction


<br>

## 2. í•™ìŠµ íšŒê³ 

* ë…¼ë¬¸ì— ëŒ€í•œ ë¸”ë¡œê·¸ ìš”ì•½ì´ë‚˜ ìœ íŠœë¸Œ ì˜ìƒì„ ë³´ì§€ ì•Šê³  ë…¼ë¬¸ ì›ë³¸ë§Œ ì½ê³  ì •ë¦¬í•œ ê²ƒì€ ì´ë²ˆì´ ì²˜ìŒì´ë¼ì„œ ì–´ë µê³  ì´í•´ê°€ ì•ˆ ë˜ëŠ” ë¶€ë¶„ì´ ë§ì•˜ë‹¤. íˆ¬ìí•œ ì‹œê°„ì— ë¹„í•´ì„œ ì´í•´í•œ ê²ƒì€ ë§ì§€ ì•Šì•˜ì§€ë§Œ ì¢‹ì€ ê²½í—˜ì´ ëœ ê²ƒ ê°™ë‹¤. ì•ìœ¼ë¡œëŠ” ì´ëŸ°ì‹ìœ¼ë¡œ ë…¼ë¬¸ ì½ëŠ” ê²ƒì— ë„ì „í•´ ë³¼ ìƒê°ì´ë‹¤ ã…ã…

<br>
