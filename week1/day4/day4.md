# Day 3 (AI Math 10 강)

## 목차 

1. [강의 내용 정리](#1-강의-내용-정리)
2. [과제 수행 과정 / 결과물 정리](#2-과제-수행-과정--결과물-정리)
3. [피어세션 정리](#3-피어세션-정리)
4. [학습 회고](#4-학습-회고)



----
안녕하세요. infj 심우창입니다.
저는 보안 동아리에 있어서 주로 시스템보안(주로 리눅스 환경에서)에 대해서 공부를 조금씩 했었습니다. 그래서 AI에 대한 지식이 별로 없습니다 ㅠㅠ
이번 1학기 캡스톤 디자인때 AI를 확욜한 악성코드 정적 탐지에 대한 것을 주제로 했었는데 제가 AI 대해서 잘 몰랐기 때문에 exe파일에서 모델 학습에 필요한 특징들을 추출하는 역할정도만 하였습니다. 이때 당시까지만 해도 인공지능에 대해서 큰 관심은 없었으나 같은 팀의 친구가 XGBoost라는 모듈을 이용해서 학습을 시키고 악성인지 정상인지 예측하는 동작이 음... 정확히 말해서 결과가 나온다는게 너무 신기하고 멋있어서 관심을 갖게 되었습니다. 비록 알고 있는게 별로 없지만 캠퍼님들 뒤에서 포기하지 않고 잘 따라가겠습니다~~!!! 

마지막으로 모든 캠퍼분들이 부스트캠프에서 성장하고 많이 배워가는 시간이 되었으면 합니다.

감사합니다~ ㅎㅎ

### 1. 강의 내용 정리

        
* AI Math 10강
    * 10강 : 동계학 맛보기
        * 지난시간 내용 - 선형모델이 풀지 못하는 문제를 풀기 위해서 비선형모델 신경망이 사용됨<br>
        

        * 소프트맥스(softmax) : 모델의 출력을 확률로 해석할 수 있게 변환해 주는 연산<br>
        &nbsp;  - 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 사용한다.<br>
        &nbsp;  - numpy를 이용하여 구현할때 소프트맥스 연산이 지수 함수를 이용한 거라서 오버플로우가 발생할 수 있으므로 이것을 예방할 수 있도록 구현해줘야 한다.<br>
        &nbsp; &nbsp;☆ 추론을 할 때는 원-핫 벡터를 이용하여 최대값을 가진 주소만 사용하기 때문에 이때는 소프트맥스를 이용하지 않음<br>

        * 신경망 - 선형모델과 활성함수(activation functiona)를 합성한 함수이다.<br>
        &nbsp;  ☆ 활성함수에서는 해당 주소?만을 실수 형태의 input으로 받는다. 그러나 소프트맥스는 전체 주소에서 벡터형태로 받는다.<br>
        &nbsp;  ☆ 활성함수를 거쳐야지 선형을 비선형으로 만들어 줄 수 있다. 그래서 딥러닝에서 매우 중요하다.<br>
        &nbsp;  - 다층 퍼셉트론 : 신경망이 여러층으로 합성된 함수<br>
        &nbsp;  - 역전파(backpropagation) <- __이것에 대해서는 좀 더 공부하고 정리하기__<br><br>

    
<br><br>

### 2. 과제 수행 과정 / 결과물 정리
<br>

#### 선택 과제1에서 ppt를 참고해서 코드를 작성했는데 뭔가 많이 이상한 것 같다. 다른 팀원들은 error가 많이 튀지 않는데 내가 짠 코드는.... 그런데 분면 ppt에서 이렇게 나왔는데 음.. 내가 어디서 실수를 했는지 잘 모르겠다. 이건 내일 질문을 통해서 해결해보고자 한다.


<br><br>


### 3. 피어세션 정리

<br>
0210805 피어세션<br><br>

🔍[이전 질문 한번더 우창님의 리뷰]<br><br>

Relu가 왜 비선형 함수인가?<br>
Relu와 sigmoid 함수의 차이<br>
KL-Divergence 한번더 공부<br>
<br><br>
entropy
cross-entropy -> 예측과 달라서 생기는 깜놀도(정보량)
https://angeloyeo.github.io/2020/10/27/KL_divergence.html
<br>
📒[금일 질문 목록]<br><br>

RNN에서 시퀀스가 너무 길면 Vanishing, Exploding -> BPTT<br><br>

Answer : 시퀀스가 너무 길때, 끝에서나 몇개의 기준을 정해서 전체에 대해 Back Prop 하지않고 정해진 기준에 대해 잘라서 거기 까지만 Back Prop 한다.
<br><br>
퀴즈 질문 -> 이 식이 어떻게 성립하는가?
<br><br>
이는 베이시안 법칙에 따라 전개하면 풀린다.
<br><br>
Variable gradient 의 개념적 이해 -> 변수 축에 따른 gradient라고 생각하면 된다.
선택과제 1번, Mini Batch에서 복원추출과 비복원추출
선택과제 2번 RNN에 대한 토의 -> grad를 어떻게 할것인가?
미분을 구할때 근사하는것이 맞는것인가, 아니면 식을 세워서 하는것이 맞는것인가?



### 4. 학습 회고

#### AI Math 10강 'RNN 첫걸음'을 들었는데 하.... 하..... 분명 이전 GD에서 backpropagation에 대해서 잘 이했했다고 생각했는데 RNN에서 backpropagation에 대한 설명을 들으니까 잘 이해가 안 되었다..... 주말에 보충으로 공부해야 되는게 조금씩 .. 이 아니라 많이 늘고 있는것 같다.
<br>

#### 오늘 모더레이터때 발표하려고 했더 정규식표현에 대해서 발표를 못 했다.... 뭔가 음.... 다른 사람들은 다 알고 있을 것 같고 분위기 자체가 AI Math나 AI 이론에 대한 것들만 이야기를 해서 말을 꺼내지 못했다 ㅠㅠ
<br>

#### 그 대신 피어세션전에 1시간 정도 공부한 KL-divergence에 대해서 발표? 설명을 했는데 KL-divergence에 대해서 제대로 이해한게 아니라서 설명이 많이 부족했던것 같다. 
<br>

#### 다음부터는 이러한 일이 없도록 이전 질문에서 내가 잘 몰랐던 것들을 전날에 공부를 하고 내 스스로 잘 이해되었을때 다 같이 공유하는 방식으로 해야겠다.

<br>

#### 아무튼 오늘도 포기하지 않고 천천히 잘 따라가서 기분이 좋다. 내일도 화이팅~👍



