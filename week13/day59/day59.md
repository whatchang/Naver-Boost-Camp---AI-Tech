<!--
구조
*
    *
        * <br>
            &nbsp; - &nbsp; <br>
                &nbsp;&nbsp;&nbsp;&nbsp; ‣ &nbsp; <br>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; * &nbsp; <br>
-->

# Day 

## 목차 

1. [공부한 내용 정리](#1-공부한-내용-정리)

2. [진행중인 실험](#2-진행중인-실험)

3. [학습 회고](#3-학습-회고)

## 1. 공부한 내용 정리

없음

<br>

## 2. 진행중인 실험

[not in-batch, random sample(use elasticsearch) train 방식](https://github.com/boostcampaitech2/mrc-level2-nlp-04/issues/36)

<br>


## 3. 학습 회고

모든 개행을 다 없애주었고 train시 토론 게시판의 글을 참고하여 transpose 부분의 오류를 수정해 주었다. 결과적으로 학습이 잘되는 것 같다.
하지만 이전에 했던 실험인 encoder model customizing과 똑같이 top - k개를 찾는 eval에서 매우 안 좋은 성능을 보여줘서 무슨 오류가 있는 것 같아 해당 eval 하는 부분의 코드를 살펴볼 예정이다.

<br>